The most common types of recommender systems can be split into collaborative filtering and content based. They can be further split into model based, and memory based.

\subsection{Content Based}
Content based recommendations require a number of features related to an item instead of historic user-item interactions. In the case of movie recommendations these could be year, actors, genres, producers, writers etc.
This method relies on calculating similarity between items using using features such as the ones previously stated.
The general idea is that if a user likes a given item he will also like items similar to it. An example of content based recommender system might use pearson or cosine similarity. One way of achieving this is by calculating term frequency (TF) and inverse document frequency (IDF) of the items. Then using the vector space model and a choice of similarity metric such as cosine or pearson, we can compare different items.

\subsection{Collaborative Filtering}
Collaborative filtering (CF) algorithms aims to recommend items to a user by combining the item interactions of a given user with item interactions of all other users. CF can be split into two categories. User-based where the aim is to measure the similarity of a given user and all other users. Item-based where we aim to measure the similarity between the items a given user has interacted with and other items.
The most widely used method of achieving this is through factorization of the very sparse user-item interaction matrix where more than 99\% of the entries are missing.

\subsection{Matrix Factorization}
The idea behind matrix factorization (MF) is to decompose the matrix R containing user-item interactions, into the product of two lower dimensional matrices P of size n X k and Q of size k X m. Matrix P is the users matrix where n is the number of users and k the number of latent factors. The other matrix is the movie matrix with m number of items and the same k latent factors.
The resulting matrices are dense and have much lower dimension than the initial matrix R. By choosing a different number of latent factors we can include more or less abstract information about the initial matrix R. MF poses the recommendation problem as an optimization one. Two common metrics used for this are root mean squared error (RMSE) and mean absolute error (MAE). The RMSE and MAE can be calculated as follows given that, \(e_i\) is the difference between actual and predicted value of rating i. 
\begin{equation}
    RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}{e^2_i}}
\end{equation}
\begin{equation}
    MAE = \frac{1}{n}\sum_{i=1}^{n}{\mid e_i\mid}
\end{equation}
% We are interested in non-negative factorization as the ratings are bound below by 0.

Singular value decomposition (SVD)
Non-negative matrix factorization (NMF)

Stochastic gradient descent (SGD) and alternating least square (ALS) are two optimization algorithm generally employed to learn a good approximation.


% One algorithm generally employed to handle this in the context of recommendations is alternating least square (ALS).
\begin{itemize}
    \item Set item factor matrix constant and adjust user factor matrix by taking the cost function derivative.
    \item Set user factor matrix constant and adjust item factor matrix.
    \item Repeat until convergence.
\end{itemize}

\subsection{Neural Networks \& Deep Learning}
Neural networks are universal approximators, typically organized in layers of neurons connected to each other through weights and put through an activation function. The number of layers and neurons at each layer also called the depth and width of the network are variable and many different configurations seem to work in practice. The simplest neural network is made up of 3 layers, input, hidden and output.

\begin{figure}[h!]
    \includegraphics[scale=0.2]{nn-3-layers}
    \caption{3 Layer Neural Network}
\end{figure}

They can be used for both supervised and unsupervised learning and can be applied to classification and regression problems just as effectively. Deep neural networks (DNN) are NN of more than 3 layers, although in practice and state of the art implementations these are many layers deep and very wide as well. The main benefit of using NNs algorithms is that they don't require manual feature engineering.
A big disadvantage is the lack of interpretability for the predictions, due to this NN are generally viewed as black boxes. This means that we are not aware of the `why` and `how` did the network product a certain output given some inputs. This could be especially detrimental in the context of recommendation systems because the users might wish to know the reasons behind their recommendations. Other memory and similarity based methods provide much more transparency in this regard.

Different classes of NN have emerged in literature including convolutional networks (CNN), autoencoders (AE), recurrent networks (RNN), generative adversarial networks (GAN) in addition of simple fully connected multilayer perceptrons (MLP).
