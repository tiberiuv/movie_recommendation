@article{HybridAutoencoders,
  author    = {Florian Strub and
               J{\'{e}}r{\'{e}}mie Mary and
               Romaric Gaudel},
  title     = {Hybrid Recommender System based on Autoencoders},
  journal   = {CoRR},
  volume    = {abs/1606.07659},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.07659},
  archivePrefix = {arXiv},
  eprint    = {1606.07659},
  timestamp = {Mon, 13 Aug 2018 16:48:42 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/StrubMG16a},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{movielens,
 author = {Harper, F. Maxwell and Konstan, Joseph A.},
 title = {The MovieLens Datasets: History and Context},
 journal = {ACM Trans. Interact. Intell. Syst.},
 issue_date = {January 2016},
 volume = {5},
 number = {4},
 month = dec,
 year = {2015},
 issn = {2160-6455},
 pages = {19:1--19:19},
 articleno = {19},
 numpages = {19},
 url = {http://doi.acm.org/10.1145/2827872},
 doi = {10.1145/2827872},
 acmid = {2827872},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Datasets, MovieLens, ratings, recommendations},
}
@Misc{Surprise,
author =   {Hug, Nicolas},
title =    { {S}urprise, a {P}ython library for recommender systems},
howpublished = {\url{http://surpriselib.com}},
year = {2017}
}
@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@misc{Adam,
  added-at = {2019-06-04T16:24:16.000+0200},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  biburl = {https://www.bibsonomy.org/bibtex/2d53bcfff0fe1a1d3a4a171352ee6e92c/alrigazzi},
  description = {Adam: A Method for Stochastic Optimization},
  interhash = {57d2ac873f398f21bb94790081e80394},
  intrahash = {d53bcfff0fe1a1d3a4a171352ee6e92c},
  keywords = {deep dl large-scale networks neural},
  note = {cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference  for Learning Representations, San Diego, 2015},
  timestamp = {2019-06-04T16:24:16.000+0200},
  title = {Adam: A Method for Stochastic Optimization},
  url = {http://arxiv.org/abs/1412.6980},
  year = 2014
}

@article{NeuralCF,
  title={Neural Collaborative Filtering},
  author={Xiangnan He and Lizi Liao and Hanwang Zhang and Liqiang Nie and Xia Hu and Tat-Seng Chua},
  journal={ArXiv},
  year={2017},
  volume={abs/1708.05031}
}

@ARTICLE{MF_Techniques, 
author={Y. {Koren} and R. {Bell} and C. {Volinsky}}, 
journal={Computer}, 
title={Matrix Factorization Techniques for Recommender Systems}, 
year={2009}, 
volume={42}, 
number={8}, 
pages={30-37}, 
keywords={information filtering;matrix decomposition;retail data processing;matrix factorization technique;recommender system;Netflix Prize competition;product recommendation system;nearest neighbor technique;Recommender systems;Motion pictures;Filtering;Collaboration;Sea measurements;Predictive models;Genomics;Bioinformatics;Nearest neighbor searches;Computational intelligence;Netflix Prize;Matrix factorization}, 
doi={10.1109/MC.2009.263}, 
ISSN={0018-9162}, 
month={Aug},}

@article{netflix_BellKor,
author = {Koren, Yehuda},
year = {2009},
month = {09},
pages = {},
title = {The BellKor solution to the Netflix Grand Prize}
}

@article{practical_training,
  author    = {Yoshua Bengio},
  title     = {Practical recommendations for gradient-based training of deep architectures},
  journal   = {CoRR},
  volume    = {abs/1206.5533},
  year      = {2012},
  url       = {http://arxiv.org/abs/1206.5533},
  archivePrefix = {arXiv},
  eprint    = {1206.5533},
  timestamp = {Mon, 13 Aug 2018 16:47:20 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1206-5533},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{initialization,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Xavier Glorot and Yoshua Bengio},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Yee Whye Teh and Mike Titterington},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {http://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@article{rectifiers,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on
               ImageNet Classification},
  journal   = {CoRR},
  volume    = {abs/1502.01852},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.01852},
  archivePrefix = {arXiv},
  eprint    = {1502.01852},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HeZR015},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}